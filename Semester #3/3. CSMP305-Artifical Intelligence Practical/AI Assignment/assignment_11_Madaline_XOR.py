# -*- coding: utf-8 -*-
"""Madaline mr I Assignment 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dUECWiV_KFw3l5skJAcTQcZQ3bTbvzOU

MADALINE MR I Learning
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

"""Madaline procedure"""

class Madaline:
    # All methods of the Madaline algorithm
    def __init__(self,
                 dataset: pd.DataFrame,
                 input_layer_size: int,
                 hidden_layer_size: int,
                 output_layer_size: int,
                 v0: float = 0.5,
                 v1: float = 0.5,
                 v2: float = 0.5,
                 bias: list = [1, 1, 1],
                 learning_rate: float = 0.5):
            """
            Initialize the Madaline network parameters.

            Args:
                dataset: Input dataset in the form of a Pandas DataFrame.
                input_layer_size: Size of the input layer.
                hidden_layer_size: Size of the hidden layer.
                output_layer_size: Size of the output layer.
                v0, v1, v2: Bias values for each layer.
                bias: List of bias terms for each hidden unit.
                learning_rate: Learning rate for weight updates.
            """
            self.v0 = v0
            self.v1 = v1
            self.v2 = v2
            self.bias = bias
            self.dataset = dataset
            self.learning_rate = learning_rate

            # Ensure learning rate is within the correct range
            if learning_rate > 1:
                    print('Learning rate exceeds 1')
                    exit()

            self.stop = 0

            # Initialize weights randomly between 0 and 1
            self.input_weights = np.random.uniform(0, 1, (input_layer_size, hidden_layer_size))
    def train(self):
            """
            Train the Madaline network on the dataset.
            """
            epoch = 0
            while not self.stop:
                    print('Epoch:', epoch)
                    epoch += 1
                    print('Input weights:', self.input_weights)
                    self.y = []  # Array to store the output for each data point

                    # Iterate through each data point in the dataset
                    for i in range(len(self.dataset)):
                            print('iteration:', i,'----------------------------')
                            # Step 5.1: Calculate net input to the hidden units
                            self.z_in = np.matmul(self.dataset.iloc[i, :-1].to_numpy(), self.input_weights)
                            print("Net input to hidden units (z_in):", self.z_in)

                            # Step 5.2: Apply the bipolar step function to get hidden layer activations
                            z_out = self.__sign_activation_function(self.z_in)
                            print("Hidden layer activations (z_out):", z_out)

                            # Step 5.3: Compute net input to the output unit
                            y_in = self.v0 + np.dot(z_out, [self.v1,self.v2])
                            print("Net input to output unit (y_in):", y_in)

                            # Step 5.4: Apply activation function to output unit
                            self.y_out = self.__sign_activation_function(y_in)
                            print("Output unit activation (y_out):", self.y_out)

                            # Store the output of the current data point
                            self.y.append(self.y_out)

                            # Step 6: Update weights if the output is incorrect
                            if self.y_out != self.dataset.iloc[i, -1]:  # Check if output matches the expected output
                                print("Output not matched!!")
                                self.__update_weight_matrix(i)

                # Check for convergence (if all outputs match target outputs)
                    self.stop = self.__stop_function()

    def __sign_activation_function(self, x):
            """
            Bipolar step activation function.

            Args:
                x: Input array.

            Returns:
                Array with elements set to 1 if >= 0, else -1.
            """
            return np.where(x >= 0, 1, -1)

    def __update_weight_matrix(self, i):
            """
            Update the weight matrix based on error correction.

            Args:
                i: Index of the current data point.
            """
        # Case I: Desired output (target) is +1
            if self.dataset.iloc[i, -1] == 1:
                    # Find hidden unit closest to zero net input and adjust weights
                    j = np.argmin(np.abs(self.z_in))
                    # Update weight for the chosen hidden unit
                    self.input_weights[:,j] = self.input_weights[:,j] + \
                                            self.learning_rate * (self.bias[j] - self.z_in[j]) * \
                                            self.dataset.iloc[i, :-1].to_numpy()
                    print('Updated weights are: ',self.input_weights)
            else:
                # Case II: Desired output is -1
                    indices = np.where(self.z_in >= 0)[0]  # Find hidden units with positive net input
                    for j in indices:
                        # Update weights for each of those units
                            self.input_weights[:,j] = self.input_weights[:,j] + \
                                                    self.learning_rate * (-self.bias[j] - self.z_in[j]) * \
                                                    self.dataset.iloc[i, :-1].to_numpy()
                            print('Updated weights are: ',self.input_weights)

    def __stop_function(self):
            """
            Check if the network has converged by comparing all outputs to targets.

            Returns:
                1 if all outputs match targets, 0 otherwise.
            """
            for i in range(len(self.dataset)):
                if self.y[i] != self.dataset.iloc[i, -1]:  # If any output is incorrect, do not stop
                    return 0
            return 1  # Stop if all outputs are correct

# Main execution
if __name__ == '__main__':
          np.random.seed(7)
          dataset = pd.read_csv('Bipolar XOR.csv')
          train_data, test_data = train_test_split(dataset, test_size=0.2,random_state=40)
          print("Training Data:")
          print(train_data)
          print("\nTesting Data:")
          print(test_data)
          mad = Madaline(train_data,input_layer_size=3,hidden_layer_size=2,output_layer_size=1)
          mad.train()

train_data, test_data = train_test_split(dataset, test_size=0.2)

print("Training Data:")
print(train_data)

print("\nTesting Data:")
print(test_data)