# -*- coding: utf-8 -*-
"""Neural Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fe9KMvfgcEfJL1wSSRq1WLpifYxg_Z29
"""

import numpy as np

class NeuralNetwork():
    def __init__(self, input_layer_size, hidden_layer_size, output_layer_size, learning_rate):
        self.input_size = input_layer_size
        self.hidden_size = hidden_layer_size
        self.output_size = output_layer_size
        #self.w1 = np.round(np.random.rand(self.input_size, self.hidden_size) - 0.5, 3)
        #self.w2 = np.round(np.random.rand(self.hidden_size, self.output_size) - 0.5, 3)
        #self.b1 = np.round(np.random.rand(1, self.hidden_size) - 0.5,3)
        #self.b2 = np.round(np.random.rand(1, self.output_size) - 0.5,3)
        self.w1 = np.array([[0.2,-0.3],[0.4, 0.1],[-0.5,0.2]])
        self.w2 = np.array([-0.3,-0.2])
        self.b1 = np.array([-0.4,0.2])
        self.b2 = np.array([0.1])
        self.learning_rate = learning_rate
        self.error_list = []
        self.limit = 0.5
        self.true_positives = 0
        self.false_positives = 0
        self.true_negatives = 0
        self.false_negatives = 0
        print("Initial Input layer to Hidden layer weights: ", self.w1)
        print("Initial Hidden layer to output layer weights: ", self.w2)
        print("Initial Biases of Hidden layer: ", self.b1)
        print("Initial Bias of Output layer: ",self.b2)
    def __sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    def __sigmoid_derivative(self, x):
        return x * (1 - x)
    def __forward(self, X):
        self.z1 = np.dot(X, self.w1) + self.b1
        self.a1 = self.__sigmoid(self.z1)
        print("Output of hidden layer: ",self.a1)
        self.z2 = np.dot(self.a1, self.w2) + self.b2
        a2 =  self.__sigmoid(self.z2)
        print("Output of output layer: ",a2)
        return a2
    def __backpropagation(self, X, target, output):
      self.o_error = (target - output)
      self.o_delta = self.o_error * self.__sigmoid_derivative(output)
      print('delta Output: ',self.o_delta)
      self.z2_error = self.o_delta*self.w2
      print('Error of output layer : ',self.z2_error)
      self.z2_delta = self.z2_error * self.__sigmoid_derivative(self.a1)
      print('Delta error of output layer: ',self.z2_delta)
      self.w1 += self.learning_rate * np.outer(X, self.z2_delta)
      # Use np.dot to get the correct dimensions for updating w2
      self.w2 += self.learning_rate * np.dot(self.a1.reshape(self.hidden_size,self.output_size), self.o_delta)
      self.b1 += self.learning_rate * self.z2_delta
      self.b2 += self.learning_rate * self.o_delta
      print('New weight of input layer to hidden layer: ',self.w1)
      print('New weight of hidden layer to output layer: ',self.w2)
      print('New baises of hidden layer: ',self.b1)
      print('New bias of output layer: ',self.b2)
    def train(self, X, y, epochs):
        for epoch in range(epochs):
            print('Epoch: ',epoch)
            o = self.__forward(X)
            self.__backpropagation(X, y, o)

    def predict(self, x_predicted):
        return self.__forward(x_predicted).item()

if __name__ == "__main__":
  n = NeuralNetwork(3,2,1,0.9)
  input = np.array([1,0,1])
  n.train(input,1,100)
  test = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]])
  for test in test:
    print('Input: ',test)
    print('Output: ',np.round(n.predict(test)))