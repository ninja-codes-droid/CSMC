# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PWNBRc2WW9FFrXgnwGUUIiAcTMFWMSl5
"""

import numpy as np
import pandas as pd
import math
from collections import Counter
import graphviz

# Dataset
data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny',
                'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild',
                    'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal',
                 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak',
             'Strong', 'Strong', 'Weak', 'Strong'],
    'PlayGolf': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes',
                 'Yes', 'No']
}

df = pd.DataFrame(data)

# Convert categorical data into numerical format
for col in df.columns:
    df[col] = df[col].astype("category").cat.codes  # Convert categories to numeric codes

# Entropy Calculation
def entropy(y):
    counter = Counter(y)
    total = len(y)
    return -sum((count/total) * math.log2(count/total) for count in counter.values())

# Information Gain Calculation
def information_gain(df, feature, target):
    total_entropy = entropy(df[target])
    values = df[feature].unique()
    weighted_entropy = sum((len(df[df[feature] == v]) / len(df)) * entropy(df[df[feature] == v][target]) for v in values)
    return total_entropy - weighted_entropy

# ID3 Algorithm - Recursively Build the Decision Tree
def id3(df, features, target, tree=None):
    # If all target values are the same, return the label
    if len(df[target].unique()) == 1:
        return df[target].iloc[0]

    # If no features left, return most common label
    if len(features) == 0:
        return df[target].mode()[0]

    # Find the best feature to split on
    best_feature = max(features, key=lambda f: information_gain(df, f, target))
    tree = {best_feature: {}}

    for value in df[best_feature].unique():
        subset = df[df[best_feature] == value]
        tree[best_feature][value] = id3(subset, [f for f in features if f != best_feature], target)

    return tree

# Build the decision tree
features = df.columns[:-1]  # All features except the target column
target = 'PlayGolf'
decision_tree = id3(df, features, target)

# Print Decision Tree
import pprint
pprint.pprint(decision_tree)

# Function to visualize the tree
def visualize_tree(tree, parent=None, graph=None):
    if graph is None:
        graph = graphviz.Digraph(format="png")

    if isinstance(tree, dict):
        for node, sub_tree in tree.items():
            if parent is None:
                graph.node(node, label=node, shape="diamond")
            else:
                graph.node(node, label=node, shape="diamond")
                graph.edge(parent, node)

            for value, branch in sub_tree.items():
                child_name = f"{node}_{value}"
                graph.node(child_name, label=str(value))
                graph.edge(node, child_name)

                visualize_tree(branch, parent=child_name, graph=graph)
    else:
        graph.node(str(tree), label=str(tree), shape="box")
        graph.edge(parent, str(tree))

    return graph

# Draw the decision tree
graph = visualize_tree(decision_tree)
graph.render("decision_tree", view=True)

import numpy as np
import pandas as pd
import math
from collections import Counter
import graphviz
from sklearn import datasets
from sklearn.preprocessing import KBinsDiscretizer

# Load the Iris dataset
iris = datasets.load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['target'] = iris.target

# Discretize Continuous Data (Convert Numerical Features into Categories)
discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
df.iloc[:, :-1] = discretizer.fit_transform(df.iloc[:, :-1])

# Rename target classes for clarity
df['target'] = df['target'].map({0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'})

# Entropy Calculation
def entropy(y):
    counter = Counter(y)
    total = len(y)
    return -sum((count/total) * math.log2(count/total) for count in counter.values())

# Information Gain Calculation
def information_gain(df, feature, target):
    total_entropy = entropy(df[target])
    values = df[feature].unique()
    weighted_entropy = sum((len(df[df[feature] == v]) / len(df)) * entropy(df[df[feature] == v][target]) for v in values)
    return total_entropy - weighted_entropy

# ID3 Algorithm - Recursively Build the Decision Tree
def id3(df, features, target, tree=None):
    # If all target values are the same, return the label
    if len(df[target].unique()) == 1:
        return df[target].iloc[0]

    # If no features left, return most common label
    if len(features) == 0:
        return df[target].mode()[0]

    # Find the best feature to split on
    best_feature = max(features, key=lambda f: information_gain(df, f, target))
    tree = {best_feature: {}}

    for value in df[best_feature].unique():
        subset = df[df[best_feature] == value]
        tree[best_feature][value] = id3(subset, [f for f in features if f != best_feature], target)

    return tree

# Build the decision tree
features = df.columns[:-1]  # All features except the target column
target = 'target'
decision_tree = id3(df, features, target)

# Print Decision Tree
import pprint
pprint.pprint(decision_tree)

# Function to visualize the tree
def visualize_tree(tree, parent=None, graph=None):
    if graph is None:
        graph = graphviz.Digraph(format="png")

    if isinstance(tree, dict):
        for node, sub_tree in tree.items():
            if parent is None:
                graph.node(node, label=node, shape="diamond")
            else:
                graph.node(node, label=node, shape="diamond")
                graph.edge(parent, node)

            for value, branch in sub_tree.items():
                child_name = f"{node}_{value}"
                graph.node(child_name, label=str(value))
                graph.edge(node, child_name)

                visualize_tree(branch, parent=child_name, graph=graph)
    else:
        graph.node(str(tree), label=str(tree), shape="box")
        graph.edge(parent, str(tree))

    return graph

# Draw the decision tree
graph = visualize_tree(decision_tree)
graph.render("iris_decision_tree", view=True)

import numpy as np
import pandas as pd
import math
from collections import Counter
import graphviz
from sklearn import datasets
from sklearn.preprocessing import KBinsDiscretizer

# Load the Iris dataset
iris = datasets.load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['target'] = iris.target

# Discretize Continuous Data (Convert Numerical Features into Categories)
discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
df.iloc[:, :-1] = discretizer.fit_transform(df.iloc[:, :-1])

# Rename target classes for clarity
df['target'] = df['target'].map({0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'})

# Entropy Calculation
def entropy(y):
    counter = Counter(y)
    total = len(y)
    return -sum((count / total) * math.log2(count / total) for count in counter.values())

# Information Gain Calculation
def information_gain(df, feature, target):
    total_entropy = entropy(df[target])
    values = df[feature].unique()
    weighted_entropy = sum((len(df[df[feature] == v]) / len(df)) * entropy(df[df[feature] == v][target]) for v in values)
    return total_entropy - weighted_entropy

# ID3 Algorithm - Recursively Build the Decision Tree
def id3(df, features, target, tree=None):
    # If all target values are the same, return the label
    if len(df[target].unique()) == 1:
        return df[target].iloc[0]

    # If no features left, return most common label
    if len(features) == 0:
        return df[target].mode()[0]

    # Find the best feature to split on
    best_feature = max(features, key=lambda f: information_gain(df, f, target))
    tree = {best_feature: {}}

    for value in df[best_feature].unique():
        subset = df[df[best_feature] == value]
        tree[best_feature][value] = id3(subset, [f for f in features if f != best_feature], target)

    return tree

# Build the decision tree
features = df.columns[:-1]  # All features except the target column
target = 'target'
decision_tree = id3(df, features, target)

# Print Decision Tree
import pprint
pprint.pprint(decision_tree)

# Function to visualize the tree with names instead of numeric values
def visualize_tree(tree, parent=None, graph=None):
    if graph is None:
        graph = graphviz.Digraph(format="png")

    if isinstance(tree, dict):
        for node, sub_tree in tree.items():
            if parent is None:
                graph.node(node, label=node, shape="diamond")
            else:
                graph.node(node, label=node, shape="diamond")
                graph.edge(parent, node)

            for value, branch in sub_tree.items():
                child_name = f"{node}_{value}"
                graph.node(child_name, label=str(value))
                graph.edge(node, child_name)

                visualize_tree(branch, parent=child_name, graph=graph)
    else:
        # When a leaf node is reached, the class label (like 'Setosa') is returned
        graph.node(str(tree), label=str(tree), shape="box")
        graph.edge(parent, str(tree))

    return graph

# Draw the decision tree
graph = visualize_tree(decision_tree)
graph.render("iris_decision_tree", view=True)

import pandas as pd
import math
from graphviz import Digraph

# Example dataset
data = {
    'age': ['youth', 'youth', 'middle_age', 'senior', 'senior', 'senior', 'middle_age', 'youth', 'youth', 'senior', 'youth', 'middle_age', 'middle_age', 'senior'],
    'income': ['high', 'high', 'high', 'medium', 'low', 'low', 'low', 'medium', 'low', 'medium', 'medium', 'medium', 'high', 'medium'],
    'student': ['no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no'],
    'credit_rating': ['fair', 'excellent', 'fair', 'fair', 'fair', 'excellent', 'excellent', 'fair', 'fair', 'fair', 'excellent', 'excellent', 'fair', 'excellent'],
    'Buy_Computer': ['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']
}

golf_play = pd.DataFrame(data)

def calculate_entropy(data, target_column):
    """
    Calculates the entropy of a dataset based on the target column.
    """
    total = len(data)
    value_counts = data[target_column].value_counts()

    entropy = 0
    for value in value_counts:
        probability = value / total
        entropy -= probability * math.log2(probability) if probability > 0 else 0

    return entropy

def calculate_information_gain(data, feature, target_column):
    """
    Calculates the information gain of splitting on a given feature.
    """
    total_entropy = calculate_entropy(data, target_column)
    feature_values = data[feature].value_counts()

    weighted_entropy = 0
    for value in feature_values.index:
        subset = data[data[feature] == value]
        weighted_entropy += (len(subset) / len(data)) * calculate_entropy(subset, target_column)

    information_gain = total_entropy - weighted_entropy
    return information_gain

def id3(data, target_column, features, graph=None, parent_node=None):
    """
    The ID3 algorithm to build a decision tree.
    """
    # Base case: if all target values are the same, return that value
    if len(data[target_column].unique()) == 1:
        return data[target_column].iloc[0]

    # Base case: if no more features to split on, return the most common target value
    if len(features) == 0:
        return data[target_column].value_counts().idxmax()

    # Calculate information gains for all features
    information_gains = []
    for feature in features:
        information_gain = calculate_information_gain(data, feature, target_column)
        information_gains.append((feature, information_gain))

    # Select the feature with the highest information gain
    best_feature = max(information_gains, key=lambda x: x[1])[0]

    # Create the tree with the best feature as the root
    if graph is None:
        graph = Digraph(format='png', engine='dot')

    node_id = str(parent_node) + str(best_feature)
    graph.node(node_id, best_feature)

    # Recurse on each subset of data for the best feature's value
    remaining_features = [f for f in features if f != best_feature]

    for value in data[best_feature].unique():
        subset = data[data[best_feature] == value]
        child_node = id3(subset, target_column, remaining_features, graph, node_id)
        child_node_id = str(node_id) + str(value)
        graph.node(child_node_id, str(value) + ' -> ' + str(child_node))
        graph.edge(node_id, child_node_id)

    return best_feature

# Define the target column and features to be used in the tree
target_column = 'Buy_Computer'
features = [col for col in golf_play.columns if col != target_column]

# Build the decision tree using ID3
graph = Digraph(format='png', engine='dot')
id3(golf_play, target_column, features, graph)

# Render and display the tree
graph.render('decision_tree', view=True)